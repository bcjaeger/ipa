% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mgboost.R
\name{mgb_predict}
\alias{mgb_predict}
\title{midy gradient boosting predictions}
\usage{
mgb_predict(object, new_data, new_n_impute, new_miss_strat,
  reshape = FALSE, predleaf = FALSE, ntreelimit = NULL,
  predcontrib = FALSE, outputmargin = FALSE, approxcontrib = FALSE,
  predinteraction = FALSE)
}
\arguments{
\item{object}{an object of class \code{xgb.Booster}.}

\item{reshape}{\code{TRUE} / \code{FALSE}. Whether to reshape the
vector of predictions to a matrix form when there are
several prediction outputs per case. This option has no
effect when either of \code{predleaf}, \code{predcontrib}, or
\code{predinteraction} flags is TRUE.}

\item{predleaf}{\code{TRUE} / \code{FALSE}. When \code{predleaf} = \code{TRUE}, the output
is a matrix object with the number of columns corresponding
to the number of trees.}

\item{ntreelimit}{The number of trees or boosting iterations
to be used when forming a sum of predicted values (see Details).
If unspecified, all trees in the ensemble will be used.}

\item{predcontrib}{\code{TRUE} / \code{FALSE}. Whether to return
contributions to individual predictions (see Details).}

\item{outputmargin}{\code{TRUE} / \code{FALSE}. If \code{TRUE}, the predictions are
returned as an untransformed sum of predictions from the boosting
ensemble. For example, setting \code{outputmargin}=\code{TRUE} for logistic
regression would result in predictions for log-odds instead of
probabilities.}

\item{approxcontrib}{\code{TRUE} / \code{FALSE}. Whether to use a fast
approximation for feature contributions (see Details).}

\item{predinteraction}{\code{TRUE} / \code{FALSE}. Whether to return
contributions of feature interactions to individual
predictions (see Details).}

\item{newdata}{an object that inherits one of the following
classes: \code{si_dmat}, \code{mi_dmat}, or \code{midy_dmat}.}
}
\value{
For regression or binary classification, it returns a vector of
length \code{nrow(newdata)}. For multiclass classification, either a
\code{num_class * nrow(newdata)} vector or a \code{(nrow(newdata), num_class)}
dimension matrix is returned, depending on the \code{reshape} value.

When \code{predleaf = TRUE}, the output is a matrix object with the
number of columns corresponding to the number of trees.

When \code{predcontrib = TRUE} and it is not a multiclass setting,
the output is a matrix object with \code{num_features + 1} columns.
The last "+ 1" column in a matrix corresponds to bias.
For a multiclass case, a list of \code{num_class} elements is returned,
where each element is such a matrix. The contribution values are on
the scale of untransformed margin (e.g., for binary classification
would mean that the contributions are log-odds deviations from bias).

When \code{predinteraction = TRUE} and it is not a multiclass setting,
the output is a 3d array with dimensions
\code{c(nrow, num_features + 1, num_features + 1)}. The off-diagonal
(in the last two dimensions) elements represent different features
interaction contributions. The array is symmetric WRT the last
two dimensions. The "+ 1" columns corresponds to bias.
Summing this array along the last dimension should produce
practically the same result as predict with \code{predcontrib = TRUE}.
For a multiclass case, a list of \code{num_class} elements is returned,
where each element is such an array.
}
\description{
Compute predictions from midy gradient boosting
decision tree ensembles.
}
\details{
Note that \code{ntreelimit} is not necessarily equal to the
number of boosting iterations and it is not necessarily equal
to the number of trees in a model. E.g., in a random forest-like
model, \code{ntreelimit} would limit the number of trees.
But for multiclass classification, while there are multiple trees
per iteration, \code{ntreelimit} limits the number of boosting iterations.

Setting \code{predcontrib = TRUE} allows to calculate contributions
of each feature to individual predictions. For "gblinear" booster,
feature contributions are simply linear terms (feature_beta *
feature_value). For "gbtree" booster, feature contributions are SHAP
values (Lundberg 2017) that sum to the difference between the expected
output of the model and the current prediction (where the hessian
weights are used to compute the expectations). Setting
\code{approxcontrib = TRUE} approximates these values following the idea
explained in \url{http://blog.datadive.net/interpreting-random-forests/}.

With \code{predinteraction = TRUE}, SHAP values of contributions of
interaction of each pair of features are computed. Note that this
operation might be rather expensive in terms of compute and memory.
Since it quadratically depends on the number of features, it is
recommended to perform selection of the most important features first.
See below about the format of the returned results.
}
\note{
This function is a wrapper for \code{predict.xgb.Booster},
an un-exported function from the \code{xgboost} package. The inputs
and details described here are copied from the documentation in
\code{predict.xgb.Booster}.
}
\references{
Scott M. Lundberg, Su-In Lee, "A Unified Approach to Interpreting
Model Predictions", NIPS Proceedings 2017,
\url{https://arxiv.org/abs/1705.07874}

Scott M. Lundberg, Su-In Lee, "Consistent feature attribution for
tree ensembles", \url{https://arxiv.org/abs/1706.06060}
}
\seealso{
\code{\link[=mgb_train]{mgb_train()}},
\code{\link[=mgb_cv]{mgb_cv()}},
and
\code{\link[=mgb_surv_prob]{mgb_surv_prob()}}
}
