---
title: "Tuning Soft Imputation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Tuning Soft Imputation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r, include = FALSE}

rm(list=ls())

knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7, 
  fig.height = 5
)


library(ipa)
library(magrittr)
library(ggplot2)
library(tidyr)
library(dplyr)
library(purrr)
library(tibble)
library(magrittr)
library(knitr)
library(kableExtra)


.theme <- theme_bw(base_size = 10) + 
  theme(
    plot.title = element_text(
      face = "bold",
      size = rel(1),
      hjust = 0.5
    ),
    text = element_text(),
    panel.background = element_rect(colour = NA),
    plot.background = element_rect(fill = "transparent",
      colour = NA),
    panel.border = element_rect(colour = "black"),
    legend.key.size = unit(3, "line"),
    legend.key.height = unit(3,"line"),
    axis.title = element_text(face = "bold",size = rel(1)),
    axis.title.y = element_text(angle = 90,vjust = 2),
    axis.title.x = element_text(vjust = -0.2),
    axis.text = element_text(size = rel(1)),
    axis.line = element_blank(),
    axis.ticks = element_line(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.key = element_rect(colour = NA),
    legend.position = "bottom",
    legend.direction = "horizontal",
    legend.title = element_text(face = "italic"),
    legend.text = element_text(size = rel(1)),
    plot.margin = unit(c(10,5, 5, 5),  "mm"),
    strip.background = element_rect(colour = "black", fill = "#f0f0f0"),
    strip.text = element_text(face = "bold")
  )

theme_set(.theme)

```

This example shows how the `midy` package works and how it may improve the accuracy of predictive models. 

## Simulation parameters

We will work with simulated data so that it is easy to assess the accuracy of multiple `xgboost` models. First, we'll load the packages we need for the analysis.

Next, we'll set simulation parameters

```{r params}

# random seed for reproducing these results
# set.seed(329)

# simulation parameters (these are explained below)
ncov = 25
nint = 0
error_sd = 1/2
tst_miss = 0
trn_miss = 9/10
nobs = 4000
split_prop = 1/2
rho = 1/2

```

As this is a simulation example, we'll need to set a few parameters, describe the context we are simulating data for, and then generate our data. Suppose that

- $Y = X\beta + \epsilon$, where $Y$ is a continuous outcome, $X$ is a matrix with `ncov` = `r ncov` covariates, $\beta$ is a vector containing coefficients that correspond to each column in $X$, and $\epsilon$ is a random error term with standard deviation = `r error_sd`. `nint` = `r nint` is the number of interactions between the columns in $X$ that impact the value of $Y$.

- Columns in $X$ follow an autoregressive correlation structure characterized by a correlation constant (`rho` = `r rho`) such that corr$(x_i, x_j)$ =  `rho`$^{\left|i-j\right|}$, where $i$ and $j$ indicate column indices in $X$.

- We sample a proportion of these data (`split_prop` = `r split_prop`) from a population of size `nobs` = `r format(nobs, big.mark=',')`. 

- A proportion (`trn_miss` = `r trn_miss`) of the values in our sample are missing at random (i.e., the missing status of a given variable is related to other variables we measured).

- We want to identify a strategy to handle these missing values that will optimize the accuracy of a gradient boosted decision tree ensemble, which will be constructed using `xgboost`. 

- We will test the accuracy of each `xgboost` model by using it to predict $Y \mid X$ among the full population (minus the observations in our training data).

In the following code, I set the parameters described above and use `gen_simdata` to create a set of 50,000 observations (with 2,500 in our training sample)

```{r}

sim_data <- gen_simdata(
  problem_type = 'regression', # continuous outcome.
  error_sd = error_sd,         # standard deviation of error.
  ncov = ncov,                 # number of predictors.
  nint = nint,                 # number of interactions.
  rho = rho,                   # autoregression constant.
  nobs = nobs,                 # total No. of observations.
  split_prop = split_prop,     # proportion used to train model.
  miss_pattern = 'mar',        # data are missing at random.
  trn_miss_prop = trn_miss,    # proportion of missing training data.
  tst_miss_prop = tst_miss     # proportion of missing training data.
)

# save the vector of beta coefficients
betas <- sim_data$beta
# remove the beta coefficients from the data object
sim_data = sim_data[-1]

trn = sim_data$training
tst = sim_data$testing

```

```{r}

naniar::vis_miss(trn)

```

The 5 steps of brewing beer and conducting imputation for predictive analytics (IPA):

## Malt

When a brew is initiated, the first step is to determine the type of wheat and prepare it for mashing. For the `ipa` package, this amounts to determining the type of imputation method and preparing it for use. In this case,

- we choose `softImpute`

- we designate `response` as the outcome. (important: outcome is not used for imputation)

```{r}

brew <- soft_malt(training_data = trn, outcome = response)

# peek at the brew contents
enframe(brew)

```

## Mash

The next step is to pair the malted wheat with hot water and mash it. In the `ipa` process, mashing is where an imputation method is paired with relevant parameters. 

```{r}

brew <- soft_mash(brew, n_impute = 10, step_size = 2)

# Note how the pars object has now been populated 
enframe(brew)

```

## Boil

During the boiling process, hops and spices are added to the wort and boiled, creating flavors for enjoyment. An `ipa` brew handles this step by adding training data to the analysis and then creating imputation models based on that data. Note that the training data has been in the `brew` object since step 1, and doesn't need to be explicitly added in this step.   

```{r}

brew <- boil(brew, verbose = TRUE, maxit = 500)

# Note how the wort now has data in it
enframe(brew)
  
```

## Ferment

To ferment the brew, yeast is added to the mixture as a catalyst. The flavors in the wort interact with the yeast and create alcohol. We ferment the `brew` object by adding data to the analysis. The imputation models interact with the data and create sets of complete (intoxicating) data. Notably, one may choose not to add `new_data`, and this would just impute the training data. However, if `new_data` is added, then one of two things will happen: 

1. The imputation models are used to fill missing cells in the testing data.

2. If the imputation models are incompatible with new data (as `softImpute` models are), then the testing data are imputed by identifying nearest neighbors in the *imputed training data.* This is not ideal, but is possibly our best option when we can't do 1.

```{r}

brew <- ferment(brew, new_data = tst)

# Note how two columns have been added to the wort
enframe(brew)

# A closer look at the wort
print(brew$wort)

```

## Bottle

The mixture is stored in bottles and aged. After this step, the brew is ready to be enjoyed!

The observed and imputed values are stored in lists and formatted as single/multiple/stacked data. After this step, the data are ready to be enjoyed (i.e., used to develop predictive models)!

```{r}

soft_dat <- bottle(brew, flavor = 'tibble')

# Bottling generally just takes whatever is 
# in the wort and adds the flavor of your choice
# (tibble or matrix) to the imputed data columns

soft_dat

```

## Pipes and brews

The brewing steps can be linked up using the handy `%>%` operator

```{r, eval = FALSE}

soft_dat <- trn %>% 
  soft_malt(outcome = response) %>% 
  soft_mash(n_impute = 10, step_size = 2) %>%
  boil(verbose = TRUE, maxit = 500) %>% 
  ferment(new_data = tst) %>% 
  bottle(flavor = 'tibble')

```

## Working with imputes

Most predictive modeling work-flows use a single imputed dataset to train models, and will also use a single imputed testing set if needed. 

Using the `bottle` function on a `brew` gives a `tibble` with lists of imputed training sets and testing sets, making it easy to identify which imputation parameters are providing the most accurate models. Let's see how we can use linear models on each of the 10 imputed datasets in our `soft_dat` object. 

```{r}
  
# function to evaluate predictions
mse <- function(x,y) mean( (x-y)^2 )

# observed outcomes in testing data
y_obs <- tst$response

# Reference mean squared error
## This is what our mean squared error would be
## if we didn't use a model and instead predicted
## values in the testing data using the mean of 
## our observed outcomes in the training data.
## We would hope that a model would do much better.

ref_mse <- mse(
  x = mean(trn$response),
  y = y_obs
)

# 1. Fit a linear model to each training set.
# 2. Compute predictions from each model in corresponding testing set.
# 3. Evaluate the mse of those predictions.
soft_lms <- map2_dbl(
  .x = soft_dat$training,
  .y = soft_dat$testing,
  .f = ~ {
    m <- lm(response ~ ., data = .x) # 1. (above) 
    p <- predict(m, newdata = .y)    # 2. (above)
    mse(p, y_obs)                    # 3. (above)
  } 
)

tbl_dat <- soft_dat %>% 
  select(impute, rank) %>% 
  mutate(
    testing_r2 = 1-soft_lms / ref_mse,
    rel_increase = 100 * (testing_r2 / testing_r2[1] - 1)
  ) %>% 
  mutate_if(is.numeric, round, 3)

kable(
  x = tbl_dat,
  col.names = c(
    'Impute', 
    'Rank',
    'Testing R-squared',
    '% change in R-squared'
  )
) %>% 
  kable_styling(bootstrap_options = c('striped','hover'))

```

It looks like using a well chosen specification of `softImpute` provides an increase in out-of-sample explained variability of `r max(tbl_dat$rel_increase)`% relative to the rank `r min(tbl_dat$rank)` `softImpute` solution. 



