---
title: "Benchmark missing strategies"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Benchmark missing strategies}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}

knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.height = 10,
  fig.width = 7
)

```

```{r setup}

library(midy)
library(recipes)
library(rsample)
library(naniar)
library(ggplot2)
library(tidyr)
library(dplyr)
library(purrr)
library(tibble)
library(pROC)
library(magrittr)
library(DescTools)
library(xgboost)

```

# Credit data

```{r}

data("credit_data", package = 'recipes')

set.seed(329)

data <- drop_na(as_tibble(credit_data)) %>% 
  initial_split(prop = 2/3, strata = Status)

derivation <- training(data)
validation <- testing(data)

library(ranger)

.eval_classif <- function(
  data_trn, 
  data_tst
){
  
  model <- ranger(
      formula = Status ~ .,
      data = data_trn,
      probability = TRUE,
      num.trees = 1000
    )
  predictions <- predict(model, data = data_tst)
  predictions <- as.numeric(predictions$predictions[,2])
  
  # Compute AUC in new data
  auc = suppressMessages(
      roc(
        response = as.numeric(data_tst$Status == 'good'),
        predictor = predictions
      ) %>% 
      use_series('auc') %>% 
      as.numeric()
  )
  
  bri = DescTools::BrierScore(
    resp = as.numeric(data_tst$Status == 'good'),
    pred = predictions,
    scaled = TRUE
  )
  
  c(auc = auc, bri = bri)
  
}


```

# Before amputation

If there were no missing values...

```{r}

performance_orig <- .eval_classif(
  data_trn = derivation, 
  data_tst = validation
)

performance_orig

```

# Amputation

Suppose our derivation set had a fairly large amount of missing data and, by some small miracle, the validation set had no missing (I'm saving that case for another vignette).

```{r}

derivation %<>% add_missing(
  omit_cols = 'Status',
  miss_proportion = .99,
  miss_pattern = 'mar'
)

vis_miss(derivation)

```

# K-nearest-neighbors

Imputation with k-nearest-neighbors has become a widely used strategy to handle missing values. The `recipes` package provides an intuitive grammar to work this step into a machine learning workflow:

```{r}

# create the recipe
reci <- recipe(Status ~ ., data = derivation) %>% 
  step_knnimpute(all_predictors(), neighbors = 5) %>%
  prep()

# create a training dataset based on the recipe above
data_trn <- juice(reci)

# create a testing dataset based on the recipe above
data_tst <- bake(reci, new_data = validation)

# See how well our glm predicts outcomes using the
# k-nearest-neighbor imputed dataset
knn_5 <- .eval_classif(data_trn, data_tst)


```

# How many neighbors?

Sometimes we need $k$ = 1 neighbor, or 5, or 10, to get an ideal set of imputed values. However, it isn't initially clear which value of $k$ will work best for this problem. The `knn_fit` function is designed to answer this question. Instead of using a single value of $k$ in its nearest neighbor routine, `knn_fit` computes imputed values for a sequence of neighbor values, and returns each corresponding data set in a `tibble`. 

## Cross-validation set up

We can combine `knn_fit` with the tools in `rsample` to run a benchmarking experiment where each value of $k$ is used to impute a dataset, and that dataset is then used as training data for a `glm`, which is finally applied to predict values in a testing (not `validation`) dataset. First we'll make the cross-validation splits:

```{r}

# set up 10-fold CV training/testing data 
# based on the derivation sample.

analysis <- vfold_cv(derivation, v = 5) %>% 
  transmute(
    # grab training/testing data from splits
    training = map(splits, ~.x$data[.x$in_id, ]),
    testing = map(splits, ~.x$data[-.x$in_id, ])
  ) 

analysis

```

And now, for each row in `analysis`,

1. apply `knn_fit` to create 20 imputed `training` sets.
2. apply `.eval_classif` to evaluate the `glm` fitted to each imputed set.

```{r}

# step 1
analysis %<>% mutate(
  cv_rslt = map2(
    .x = training, 
    .y = testing,
    .f = ~ knn_fit( 
      data = .x, 
      outcome = 'Status', 
      n_impute = 20, 
      step_size = 2,
      verbose = FALSE
    )
  )
)
  
analysis

```


```{r}

# step 2
mm_impute <- function(data){
  imp_vals <- map(data, .f = ~ {
    if(is.factor(.x)){
      tb = table(.x)
      names(tb)[which.max(tb)]
    } else {
      mean(.x, na.rm = TRUE)
    }
  })
  for(i in names(data)){
    data[[i]][is.na(data[[i]])] <- imp_vals[[i]]
  }
  data
}


cv_rslts <- analysis %>% 
  select(cv_rslt, testing) %>% 
  unnest(cols = cv_rslt) %>% 
  mutate(
    eval = map2(
      .x = data,
      .y = testing, 
      .f = ~.eval_classif(.x, mm_impute(.y))
    )
  )

cv_smry <- cv_rslts %>% 
  unnest_wider(col = eval) %>% 
  select(impute, neighbors, bri, auc) %>% 
  group_by(neighbors) %>% 
  summarize_all(mean)

cv_smry 

```

Choosing the neighbor count that maximized cross-validated AUC:

```{r}

neighbor_vals <- cv_smry %>%
  arrange(desc(auc)) %>% 
  dplyr::slice(1:5) %>% 
  pull(neighbors)

neighbor_cv <- neighbor_vals[1]

reci <- recipe(Status ~ ., data = derivation) %>% 
  step_knnimpute(all_predictors(), neighbors = neighbor_cv) %>%
  prep()

data_trn <- juice(reci)
data_tst <- bake(reci, new_data = validation)

knn_cv <- .eval_classif(data_trn, data_tst)

```

# Multiple imputation

```{r}

knn_mi <- knn_fit(
  data = derivation, 
  outcome = 'Status', 
  neighbor_sequence = neighbor_vals
)

mdl_mi <- map(
  .x = knn_mi$data,
  .f = ~ glm(Status ~ ., family = binomial, 
    data = select(.x, -ends_with('imp')))
)

prd_mi <- map_dfc(
  .x = mdl_mi, 
  .f = predict, 
  newdata = validation,
  type = 'response'
) %>%
  apply(1, mean)

knn_ens <- c(
  auc = roc(
    response = validation$Status=='good', 
    predictor = prd_mi
  ) %>% 
    use_series("auc") %>% 
    as.numeric(),
  bri = BrierScore(
    resp = as.numeric(validation$Status=='good'), 
    pred = prd_mi, 
    scale = TRUE
  )
)

knn_ens




```

# Summary

This article has shown how to use `fit` and `fill` functions to experiment with different strategies to handle missing data and then apply that strategy in a modeling workflow. Results that we have shown along the way demonstrate how careful selection of hyper-parameters for missing data algorithms (i.e., k-nearest-neighbors) and ensembling or stacking imputed datasets can help to increase the accuracy of your model's predictions. The table below summarizes the cases we covered: 


```{r}

fnl <- list(
  `Original data (no missing)` = performance_orig,
  `Default k-nearest neighbor imputation` = knn_5,
  `Tuning k with cross-validation` = knn_cv,
  `Ensembling the top 5 values of k` = knn_ens
) %>% 
  map(as_tibble) %>% 
  map(mutate, variable = c("AUC", "Brier Score (scaled)")) %>% 
  bind_rows(.id = 'Missing strategy') %>% 
  mutate(`Missing strategy` = forcats::fct_inorder(`Missing strategy`)) %>% 
  spread(variable, value)

fnl

```


<!-- # Data stacking -->

<!-- Pooling models is fundamentally important for statistical inference, but isn't a necessity when it comes to predictive models. In fact, it could be argued that stacking data and fitting one predictive model makes it *easier* to interpret the model compared to interpreting an ensemble of models. Additionally, a model based on stacked data might just be more prognostic than an ensemble of models based on separate imputations. -->

```{r}

# mdl_stk <- bind_rows(knn_mi$data)
# 
# knn_stk <- .eval_classif(mdl_stk, data_tst)

```
