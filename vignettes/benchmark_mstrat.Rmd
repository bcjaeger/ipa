---
title: "Benchmark missing strategies"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Benchmark missing strategies}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}

knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.height = 10,
  fig.width = 7
)

```

```{r setup}

library(midy)
library(recipes)
library(rsample)
library(naniar)
library(ggplot2)
library(tidyr)
library(dplyr)
library(purrr)
library(tibble)
library(pROC)
library(magrittr)
library(DescTools)
library(xgboost)

```

# Credit data

```{r}

data("credit_data", package = 'recipes')

set.seed(329)

data <- drop_na(as_tibble(credit_data)) %>% 
  initial_split(prop = 2/3, strata = Status)

derivation <- training(data)
validation <- testing(data)

```

## Evaluating missing strategies

We want to assess a number of different approaches to impute data. However, we aren't going to evaluate the accuracy of the imputation, because who really cares about accurately imputing a predictor? For example, suppose you are visiting your doctor and you ask her to predict your risk for a heart attack in the next 10 years. Your doctor then asks you to provide your age, race, sex, and income to help her formulate a risk prediction. You decide to tell her your age, race, and sex, but don't feel comfortable sharing your income. In this scenario, which of these two things seems more important to you?

- A: Your doctor accurately predicts your income, even though you didn't want to share it (rude).

- B: Your doctor accurately predicts your risk of a heart attack, even though you didn't provide all the information she asked for. 

I'd be impressed if my doctor could do both, but all I really care about is how well B is addressed. Therefore, in this example, imputation strategies are evaluated based on the *predictive accuracy of a model fitted to the imputed data*. To make sure these comparisons are fair, we fix the model fitting algorithm and only vary the strategies used to handle missing values.

## Helper function

Since all aspects of model fitting are fixed, we can automate these steps in a function: `.eval_classif()`. This function takes training/testing data as input and, after fitting an `xgboost` model to the training data, evaluates it in the testing data.

```{r}

.eval_classif <- function(
  data_trn,
  data_tst
){

  params = list(
    max_depth = 2,
    subsample = 1/2,
    colsample_bynode = 1/2,
    min_child_weight = 10,
    eta = 0.05,
    objective = 'binary:logistic',
    eval_metric = 'auc'
  )

  label_trn <- as.numeric(data_trn$Status)-1

  dmat_trn <- data_trn %>%
    spread_cats() %>%
    select(-contains("Status")) %>%
    as.matrix() %>%
    xgb.DMatrix(label = label_trn)

  dmat_tst <- data_tst %>%
    transfer_factor_levels(from = data_trn) %>%
    spread_cats() %>%
    select(-contains("Status")) %>%
    as.matrix()

  cv <- xgb.cv(
    data = dmat_trn,
    params = params,
    nfold = 10,
    nrounds = 1000,
    verbose = 0,
    early_stopping_rounds = 50
  )

  fit <- xgb.train(
    params = params,
    data = dmat_trn,
    nrounds = cv$best_iteration,
    verbose = 0
  )

  predictions <- predict(fit, newdata = dmat_tst)

  # Compute AUC in new data
  auc = suppressMessages(
    roc(
      response = as.numeric(data_tst$Status == 'good'),
      predictor = predictions
    ) %>%
      use_series('auc') %>%
      as.numeric()
  )

  # Compute scaled brier score in new data
  bri = DescTools::BrierScore(
    resp = as.numeric(data_tst$Status == 'good'),
    pred = predictions,
    scaled = TRUE
  )

  c(auc = auc, bri = bri)

}


```

# Data amputation

If there were no missing values, our model assessment would be pretty good!

```{r}

performance_orig <- .eval_classif(
  data_trn = derivation, 
  data_tst = validation
)

performance_orig

```

Unfortunately, most data sets do not come without missing values. In this example, we'll consider a scenario where almost all participants in the data are missing some values (i.e., `miss_proportion` = `0.99`), and the missing status of these variables occurs completely at random (i.e., `miss_pattern` = `mcar`).

```{r}

derivation %<>% add_missing(
  omit_cols = 'Status',
  miss_proportion = .99,
  miss_pattern = 'mcar'
)

validation %<>% add_missing(
  omit_cols = 'Status',
  miss_proportion = .99,
  miss_pattern = 'mcar'
)

```

Let's take a look at the overall missing pattern in the `derivation` data.

```{r}

vis_miss(derivation)

```

A little less than half of the values in `derivation` are missing (similar proportion are missing in `validation`). Let's see how well our classifier performs using the standard `xgboost` approach to handle missing values.

```{r}

performance_default_xgb <- .eval_classif(derivation, validation)

performance_default_xgb

```

Ouch. That is a serious drop in prediction accuracy. Is there anything we can do to recover some of the signal that was lost in our missing values?

# K-nearest-neighbors

Imputation with k-nearest-neighbors has become a widely used strategy to handle missing values. The `recipes` package provides an intuitive grammar to work this step into a machine learning workflow:

```{r}

# create the recipe
# note that 5 neighbors is a default value. 
reci <- recipe(Status ~ ., data = derivation) %>%
  step_knnimpute(all_predictors(), neighbors = 5) %>%
  prep()

# create a training dataset based on the recipe above
data_trn <- juice(reci)

# create a testing dataset based on the recipe above
data_tst <- bake(reci, new_data = validation)

# See how well our random forest predicts outcomes using the
# k-nearest-neighbor imputed dataset
performance_knn_5 <- .eval_classif(data_trn, data_tst)


# Here is how our classifiers stand 
performance_orig
performance_default_xgb
performance_knn_5

```

From the results above, it seems that using 5 neighbors with the `knn` imputation algorithm helps return some important information to the `derivation` and `validation` data. Can we do better if we carefully select the number of neighbors used by the `knn` algorithm? 

# How many neighbors?

Sometimes we need $k$ = 1 neighbor, or 5, or 10, to get an ideal set of imputed values. However, it isn't initially clear which value of $k$ will work best for this problem. The `knn_fit` function is designed to answer this question. Instead of using a single value of $k$ in its nearest neighbor routine, `knn_fit` computes imputed values for a sequence of neighbor values, and returns each corresponding data set in a `tibble`. 

## Cross-validation set up

We can combine `knn_fit` with the tools in `rsample` to run a benchmarking experiment where each value of $k$ is used to impute a dataset, and that dataset is then used as training data for a `glm`, which is finally applied to predict values in a testing (not `validation`) dataset. First we'll make the cross-validation splits:

```{r}

# set up 10-fold CV training/testing data
# based on the derivation sample.

analysis <- vfold_cv(derivation, v = 5) %>%
  transmute(
    # grab training/testing data from splits
    training = map(splits, ~.x$data[.x$in_id, ]),
    testing = map(splits, ~.x$data[-.x$in_id, ])
  )

analysis

neighbor_seq <- seq(1, 20)

```

And now, for each row in `analysis`,

1. apply `knn_fit` to create 20 imputed `training` sets.

2. apply `knn_fill` to create one imputed testing set for each imputed training set. 

3. apply `.eval_classif` to evaluate the `xgboost` model fitted to each imputed pair of training/testing data.

**Note** this will take about 25 minutes to run! Your patience will pay off.

```{r}

# step 1
analysis %<>% mutate(
    trn = map(
      .x = training,
      .f = ~ knn_fit(
          data = .x,
          outcome = 'Status',
          neighbor_sequence = neighbor_seq,
          verbose = FALSE
        )
    )
  )

analysis

```

Code for step 2 follows:

```{r}

# step 2

cv_rslts <- analysis %>%
  select(trn, tst = testing) %>%
  unnest(cols = trn) %>% 
  mutate(neighbors = rep(neighbor_seq, 5))

# peek at the unnested data

cv_rslts

# impute each test set using the 
# given number of neighbors

cv_rslts %<>%
  mutate(
    tst = pmap(
      .l = list(trn, neighbors, tst),
      .f = function(.trn, .n, .tst){
        knn_fill(
          data = .trn,
          new_data = .tst,
          outcome = 'Status',
          neighbor_sequence = .n,
          verbose = FALSE
        )
      }
    )
  )

cv_rslts

```

Code for step 3 follows:

```{r}

cv_smry <- cv_rslts %>%
  mutate(
    tst = map(tst, ~.x[[1]]),
    eval = map2(
      .x = trn,
      .y = tst,
      .f = .eval_classif
    )
  ) %>%
  unnest_wider(col = eval) %>%
  select(neighbors, bri, auc) %>%
  group_by(neighbors) %>%
  summarize_all(mean) %>%
  arrange(desc(auc))

cv_smry

```

Now we can choose a neighbor count that maximizes our cross-validated AUC (note: AUC = area under the receiver operating characteristic curve) 

```{r}

neighbor_vals <- cv_smry %>%
  dplyr::slice(1:5) %>% 
  pull(neighbors)

neighbor_cv <- neighbor_vals[1]

# Re-do knn imputation using the neighbor 
# count selected with cross-validation

reci <- recipe(Status ~ ., data = derivation) %>% 
  step_knnimpute(all_predictors(), neighbors = neighbor_cv) %>%
  prep()

data_trn <- juice(reci)
data_tst <- bake(reci, new_data = validation)

performance_knn_cv <- .eval_classif(data_trn, data_tst)

performance_knn_cv
performance_knn_5

```

And now we can see that using cross-validation to select $k$ has improved the prediction accuracy of our model once again. But model performance can still be improved by ensembling multiple datasets for prediction, similar to the way that predictive models can be combined to form an ensemble. (This approach is more commonly known as multiple imputation.)  

# Multiple imputation

Suppose that instead of picking the 'best' strategy to impute data, we pick a set of 'good' strategies and use each of them, separately, to fit a model. Then, to make a prediction for new data, we compute each model's predicted probability and take a simple average (similar to a democratic vote)

```{r}

# Multiple imputation
mi_trn <- knn_fit(
  data = derivation,
  outcome = 'Status',
  neighbor_sequence = neighbor_vals,
  verbose = FALSE
)

mi_tst <- knn_fill(
  data = derivation,
  new_data = validation,
  outcome = 'Status',
  neighbor_sequence = neighbor_vals,
  verbose = FALSE
)

mi_mdls <- map2(
  .x = mi_trn,
  .y = mi_tst,
  .f = ~ {
    
    params = list(
      max_depth = 2,
      subsample = 1/2,
      colsample_bynode = 1/2,
      min_child_weight = 10,
      eta = 0.05,
      objective = 'binary:logistic',
      eval_metric = 'auc'
    )
    
    label_trn <- as.numeric(.x$Status)-1
    
    dmat_trn <- .x %>%
      spread_cats() %>%
      select(-contains("Status")) %>%
      as.matrix() %>%
      xgb.DMatrix(label = label_trn)
    
    dmat_tst <- .y %>%
      transfer_factor_levels(from = .x) %>%
      spread_cats() %>%
      select(-contains("Status")) %>%
      as.matrix()
    
    cv <- xgb.cv(
      data = dmat_trn,
      params = params,
      nfold = 10,
      nrounds = 1000,
      verbose = 0,
      early_stopping_rounds = 50
    )
    
    xgb.train(
      params = params,
      data = dmat_trn,
      nrounds = cv$best_iteration,
      verbose = 0
    )
    
  }
)

mi_dmat_val <- map2(
  .x = mi_trn,
  .y = mi_tst,
  .f = ~ .y %>%
    transfer_factor_levels(from = .x) %>%
    spread_cats() %>%
    select(-contains("Status")) %>%
    as.matrix() %>%
    xgb.DMatrix()
)

prd_mi <- map2_dfc(
  .x = mi_mdls,
  .y = mi_dmat_val,
  .f = ~ predict(.x, newdata = .y),
) %>%
  apply(1, mean)

performance_knn_ens <- c(
  auc = roc(
    response = validation$Status=='good',
    predictor = prd_mi
  ) %>%
    use_series("auc") %>%
    as.numeric(),
  bri = BrierScore(
    resp = as.numeric(validation$Status=='good'),
    pred = prd_mi,
    scale = TRUE
  )
)

performance_knn_ens




```

That is a hefty bump up in both the discrimination and scaled brier score of the model. Notably, the computational resources we needed to run cross-validation were high. However, cross-validation is not too hard to run in parallel. Also, we could have applied more of the ensemble learning theory to this problem and determined an optimal set of weights to sum our model's predictions with. But, to keep the vignette from getting too long, I didn't want to go there.  

# Summary

This article has shown how to use `fit` and `fill` functions to experiment with different strategies to handle missing data and then apply that strategy in a modeling workflow. Results have shown how careful selection of hyper-parameters for missing data algorithms (i.e., k-nearest-neighbors) and ensembling imputed datasets can help to increase the accuracy of your model's predictions. The table below summarizes the case we covered: 


```{r}

fnl <- list(
  `Original data (no missing)` = performance_orig,
  `Default xgboost strategy for missing data` = performance_default_xgb,
  `Default k-nearest neighbor imputation` = performance_knn_5,
  `Tuning k with cross-validation` = performance_knn_cv,
  `Ensembling the top 5 values of k` = performance_knn_ens
) %>% 
  map(as_tibble) %>% 
  map(mutate, variable = c("AUC", "Brier Score (scaled)")) %>% 
  bind_rows(.id = 'Missing strategy') %>% 
  mutate(`Missing strategy` = forcats::fct_inorder(`Missing strategy`)) %>% 
  spread(variable, value)

fnl

```


## More results

But what if the example we just looked at was a fluke? Perhaps the way the data were split just happened to make the methods I wanted to investigate look good. I ran the same experiment 25 times, each time using a different split of the data, and compiled the results in a dataframe that loads along with the `midy` package. 

```{r}

library(knitr)
library(kableExtra)

data("resamp_credit_rslts")

resamp_credit_rslts %>%
  group_by(mstrat) %>%
  summarize_if(is.numeric, mean) %>% 
  kable(
    col.names = c(
      "Missing data strategy",
      "AUC", 
      "Brier Score (scaled)"
    )
  ) %>% 
  kable_styling(bootstrap_options = c("striped","hover")) %>% 
  footnote(general = 'k indicates the number of nearest neighbors')


```

# Appendix 

In case you would like to run this simulation for yourself, the code I used to get results in `resampl_credit_rslts` is pasted below:

```{r, eval = FALSE}

library(midy)
library(recipes)
library(rsample)
library(naniar)
library(ggplot2)
library(tidyr)
library(dplyr)
library(purrr)
library(tibble)
library(pROC)
library(magrittr)
library(DescTools)
library(xgboost)
library(readr)

.eval_classif <- function(
  data_trn,
  data_tst
){

  params = list(
    max_depth = 2,
    subsample = 1/2,
    colsample_bynode = 1/2,
    min_child_weight = 10,
    eta = 0.05,
    objective = 'binary:logistic',
    eval_metric = 'auc'
  )

  label_trn <- as.numeric(data_trn$Status)-1

  dmat_trn <- data_trn %>%
    spread_cats() %>%
    select(-contains("Status")) %>%
    as.matrix() %>%
    xgb.DMatrix(label = label_trn)

  dmat_tst <- data_tst %>%
    transfer_factor_levels(from = data_trn) %>%
    spread_cats() %>%
    select(-contains("Status")) %>%
    as.matrix()

  cv <- xgb.cv(
    data = dmat_trn,
    params = params,
    nfold = 10,
    nrounds = 1000,
    verbose = 0,
    early_stopping_rounds = 50
  )

  fit <- xgb.train(
    params = params,
    data = dmat_trn,
    nrounds = cv$best_iteration,
    verbose = 0
  )

  predictions <- predict(fit, newdata = dmat_tst)

  # Compute AUC in new data
  auc = suppressMessages(
    roc(
      response = as.numeric(data_tst$Status == 'good'),
      predictor = predictions
    ) %>%
      use_series('auc') %>%
      as.numeric()
  )

  bri = DescTools::BrierScore(
    resp = as.numeric(data_tst$Status == 'good'),
    pred = predictions,
    scaled = TRUE
  )

  c(auc = auc, bri = bri)

}

data("credit_data", package = 'recipes')

set.seed(329)

niter = 25

rslts <- vector(mode = 'list', length = niter)

for(i in seq(niter)){

  data <- drop_na(as_tibble(credit_data)) %>%
    initial_split(prop = 2/3, strata = Status)

  derivation <- training(data)
  validation <- testing(data)

  performance_orig <- .eval_classif(
    data_trn = derivation,
    data_tst = validation
  )

  # Amputation

  derivation %<>% add_missing(
    omit_cols = 'Status',
    miss_proportion = .99,
    miss_pattern = 'mcar'
  )

  validation %<>% add_missing(
    omit_cols = 'Status',
    miss_proportion = .99,
    miss_pattern = 'mcar'
  )

  # xgboost default

  performance_default_xgb <- .eval_classif(derivation, validation)

  # K-nearest-neighbors

  # create the recipe
  reci <- recipe(Status ~ ., data = derivation) %>%
    step_knnimpute(all_predictors(), neighbors = 5) %>%
    prep()

  # create a training dataset based on the recipe above
  data_trn <- juice(reci)

  # create a testing dataset based on the recipe above
  data_tst <- bake(reci, new_data = validation)

  # See how well our random forest predicts outcomes using the
  # k-nearest-neighbor imputed dataset
  knn_5 <- .eval_classif(data_trn, data_tst)

  # set up 10-fold CV training/testing data
  # based on the derivation sample.

  analysis <- vfold_cv(derivation, v = 5) %>%
    transmute(
      # grab training/testing data from splits
      training = map(splits, ~.x$data[.x$in_id, ]),
      testing = map(splits, ~.x$data[-.x$in_id, ])
    )

  neighbor_seq <- seq(1, 20)

  # step 1
  analysis %<>% mutate(
    trn = map(
      .x = training,
      .f = ~ knn_fit(
          data = .x,
          outcome = 'Status',
          neighbor_sequence = neighbor_seq,
          verbose = FALSE
        )
    )
  )

  # step 2

  cv_rslts <- analysis %>%
    select(trn, tst = testing) %>%
    unnest(cols = trn) %>%
    mutate(
      neighbors = rep(neighbor_seq, 5),
      tst = pmap(
        .l = list(trn, neighbors, tst),
        .f = function(.trn, .n, .tst){
          knn_fill(
            data = .trn,
            new_data = .tst,
            outcome = 'Status',
            neighbor_sequence = .n,
            verbose = FALSE
          )
        }
      )
    )

  tmp <- cv_rslts %>%
    mutate(
      tst = map(tst, ~.x[[1]]),
      eval = map2(
        .x = trn,
        .y = tst,
        .f = .eval_classif
      )
    )

  cv_smry <- tmp %>%
    unnest_wider(col = eval) %>%
    select(neighbors, bri, auc) %>%
    group_by(neighbors) %>%
    summarize_all(mean) %>%
    arrange(desc(auc))

  neighbor_vals <- cv_smry %>%
    dplyr::slice(1:5) %>%
    pull(neighbors)

  neighbor_cv <- neighbor_vals[1]

  reci <- recipe(Status ~ ., data = derivation) %>%
    step_knnimpute(all_predictors(), neighbors = neighbor_cv) %>%
    prep()

  data_trn <- juice(reci)
  data_tst <- bake(reci, new_data = validation)

  knn_cv <- .eval_classif(data_trn, data_tst)


  # Multiple imputation
  mi_trn <- knn_fit(
    data = derivation,
    outcome = 'Status',
    neighbor_sequence = neighbor_vals,
    verbose = FALSE
  )

  mi_tst <- knn_fill(
    data = derivation,
    new_data = validation,
    outcome = 'Status',
    neighbor_sequence = neighbor_vals,
    verbose = FALSE
  )

  mi_mdls <- map2(
    .x = mi_trn,
    .y = mi_tst,
    .f = ~ {

      params = list(
        max_depth = 2,
        subsample = 1/2,
        colsample_bynode = 1/2,
        min_child_weight = 10,
        eta = 0.05,
        objective = 'binary:logistic',
        eval_metric = 'auc'
      )

      label_trn <- as.numeric(.x$Status)-1

      dmat_trn <- .x %>%
        spread_cats() %>%
        select(-contains("Status")) %>%
        as.matrix() %>%
        xgb.DMatrix(label = label_trn)

      dmat_tst <- .y %>%
        transfer_factor_levels(from = .x) %>%
        spread_cats() %>%
        select(-contains("Status")) %>%
        as.matrix()

      cv <- xgb.cv(
        data = dmat_trn,
        params = params,
        nfold = 10,
        nrounds = 1000,
        verbose = 0,
        early_stopping_rounds = 50
      )

      xgb.train(
        params = params,
        data = dmat_trn,
        nrounds = cv$best_iteration,
        verbose = 0
      )

    }
  )

  mi_dmat_val <- map2(
    .x = mi_trn,
    .y = mi_tst,
    .f = ~ .y %>%
      transfer_factor_levels(from = .x) %>%
      spread_cats() %>%
      select(-contains("Status")) %>%
      as.matrix() %>%
      xgb.DMatrix()
  )

  prd_mi <- map2_dfc(
    .x = mi_mdls,
    .y = mi_dmat_val,
    .f = ~ predict(.x, newdata = .y),
  ) %>%
    apply(1, mean)

  knn_ens <- c(
    auc = roc(
      response = validation$Status=='good',
      predictor = prd_mi
    ) %>%
      use_series("auc") %>%
      as.numeric(),
    bri = BrierScore(
      resp = as.numeric(validation$Status=='good'),
      pred = prd_mi,
      scale = TRUE
    )
  )

  rslts[[i]] <- list(
    `Original data (no missing)` = performance_orig,
    `Default xgboost missing data strategy` = performance_default_xgb,
    `Default k-nearest neighbor imputation` = knn_5,
    `Tuning k with cross-validation` = knn_cv,
    `Ensembling the top 5 values of k` = knn_ens
  ) %>%
    map(as_tibble) %>%
    map(mutate, variable = c('auc','sbs')) %>%
    bind_rows(.id = 'Missing strategy') %>%
    mutate(`Missing strategy` = forcats::fct_inorder(`Missing strategy`)) %>%
    spread(variable, value)

  print(i)
  print(rslts[[i]])

}
```





<!-- # Data stacking -->

<!-- Pooling models is fundamentally important for statistical inference, but isn't a necessity when it comes to predictive models. In fact, it could be argued that stacking data and fitting one predictive model makes it *easier* to interpret the model compared to interpreting an ensemble of models. Additionally, a model based on stacked data might just be more prognostic than an ensemble of models based on separate imputations. -->

```{r}

# mdl_stk <- bind_rows(knn_mi$data)
# 
# knn_stk <- .eval_classif(mdl_stk, data_tst)

```
