---
title: "Getting started with `ipa`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{getting_started}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Overview

The `ipa` package helps create multiply imputed datasets using a specific method. The datasets are differentiated by specific tuning parameters that correspond to the method. Analysts can identify a single missing data strategy that works best or develop an ensembling method to combine different data strategies. 

True to its name, `ipa` has a workflow based around the process of brewing. The main steps involved are

1. `brew` Create a container to hold your imputations

2. `spice` set parameters that govern the number of imputations for the given brew

3. `mash` set secondary parameters for imputation and fit models to the training data.

4. `ferment` impute missing values in training and (optionally) testing data 

5. `bottle` output the imputed datasets in a tibble or matrix format.

# Diabetes data

These data are courtesy of Dr John Schorling, Department of Medicine, University of Virginia School of Medicine. The data originally described 1046 participants who were interviewed in a study to understand the prevalence of obesity, diabetes, and other cardiovascular risk factors in central Virginia for African Americans. A total of 403 participants were screened for diabetes, which was defined as glycosolated hemoglobin > 7.0. Among those 403 participants, 366 provided complete data for the variables in these data.

We'll work with the complete (`diab_complete`) and incomplete (`diab_incomplete`; , missingness added artificially and completely at random) versions of the `diabetes` data here, and we'll focus on developing a prediction model to classify whether someone has prevalent type II diabetes based on clinical risk factors. 

```{r}

library(ipa)
library(dplyr)
library(purrr)
library(rsample)
library(parsnip)
library(rsample)
library(ggplot2)
library(yardstick)

# random seed for reproducing these results
set.seed(329)

data("diabetes", package = 'ipa')

diab_complete <- diabetes$complete
diab_missing <- diabetes$missing

```

# Problem description

A common task for prediction model development is handling missing values. `ipa` helps analysts access more options as they engage with this task. This vignette shows some of `ipa`'s features in the context of imputing missing values in the `diabetes` data. Here is a glimpse of the complete version of the data:

```{r}

glimpse(diab_complete)

```

And here is the amputed data, missing roughly 40% of the original values.

```{r}

glimpse(diab_missing)

```

We'll work with a training set and testing set in this example:

```{r}

split <- initial_split(diab_missing, prop = 1/2, strata = 'diabetes')

split

```


# Brew

To start your workflow with `ipa`, initiate a brew using `brew_<flavor>`. The object created is essentially a list containing meta data about your missing data strategy. However, printing your `brew` object will always show the original data, because that's usually more helpful than overwhelming the console with all of the list elements. 

```{r}

diab_brew <- brew_nbrs(data = training(split), outcome = diabetes)

diab_brew

```

Here we use `brew_nbrs` to create `diab_brew`. This function initiates an object that will use k-nearest neighbors to impute missing values. Other brewing functions include `brew_soft` for soft imputation and `brew_rngr` for imputation using `missRanger`.

# Spice

`spice` lets you control how many datasets are created and the primary tuning parameter for the given imputation flavor. For nearest neighbor imputation, the primary tuning parameter is the number of nearest neighbors.

The `spice` function has an argument called `with` that depends on your brew's flavor. Basically, you want to use `with = spicer_<flavor>` for a `<flavor>` brew. Following that recipe, we use `spicer_nbrs` for our `nbrs` brew.

```{r}

spcd_brew <- spice(diab_brew, with = spicer_nbrs(neighbors = c(1:50)))

# take a look at the brew's parameters (what we added using spice)
spcd_brew$pars

```

# Mash

`mash` allows you to set secondary parameters for imputing data with your brew. Additionally, `mash` will fit the models needed to impute the `brew`'s data. For a nearest neighbors brew, we can set the function that will be used to aggregate neighbors with continuous, integer, or categorical data using `masher_nbrs`. Just like `spice`, we want to use `masher_<flavor>` for a `<flavor>` brew.

```{r}

mshd_brew <- mash(spcd_brew, with = masher_nbrs(fun_aggr_ctns = median))

mshd_brew$wort

```

The `brew`'s `wort` will be filled with imputation models (i.e., the column called `fit`) after `mash`ing. For nearest neighbors imputation, the `fit` is just the training data.

```{r}

mshd_brew$wort

```

# Ferment

`ferment` imputes missing values in testing data using models fitted to training data. `ipa` tries to adhere to a few principles when it comes to imputing missing values in testing data: 

1. Outcome columns should not be used to impute missing values in training data. The outcome column will naturally be missing in the testing data, so any imputation model that depends on accessing the outcome column will be unable to impute missing values in testing data.

2. Missing data strategies should use the training data and only the training data to impute missing values in the testing data. Some imputation strategies cannot do this by design, e.g. `softImpute`, so `ipa` applies a workaround (`ferment_stk`; see below).

3. The same strategy that was used to impute training data should also be used to impute testing data.

Following these principles (and making exceptions when needed), `ipa` has two broad strategies to implement.

**Impute with fit** (`ferment_fit`) Models developed from the training data are applied to create imputations for missing values in the testing data.

**Stack and fit:** (`ferment_stk`) Training and testing data are stacked. Imputation models are re-fitted to the stacked dataset. This strategy works for any imputation method, but may require substantially more time to run. 

```{r}

# quicker
start <- Sys.time()
ferm_brew_fit <- ferment_fit(mshd_brew, new_data = testing(split))
stop <- Sys.time()

print(stop - start)


start <- Sys.time()
ferm_brew_stk <- ferment_stk(mshd_brew, new_data = testing(split))
stop <- Sys.time()

print(stop - start)

```

# Bottle

Once everything is imputed, your brew is ready to be bottled. You can choose whether you'd prefer to get the data back in the form of a `tibble` or `matrix`.

```{r}

imputes <- bottle(ferm_brew_fit, type = 'tibble')

imputes

```

# Pipes and brews

The workflow functions in `ipa` are designed to be used with the `%>%` operator. 

```{r, eval = FALSE}

brew_nbrs <- training(split) %>% 
  brew_nbrs(outcome = diabetes) %>% 
  spice(neighbors = 1:50) %>% 
  mash(with = masher_nbrs(fun_aggr_ctns = median)) %>% 
  ferment_fit(new_data = testing(split)) %>% 
  bottle(type = 'tibble')

```

# Modeling

We will fit a `parsnip` logistic regression model to imputed training sets and then compute predictions on the testing sets.

```{r}

# a function to fit logit model, and predict on new data
fit_logit <- function(df_train, df_test){
  logistic_reg() %>% 
    set_engine('glm') %>% 
    fit(diabetes ~ ., data = df_train) %>% 
    predict(new_data = df_test, type = 'prob')
}

brew_nbrs <- brew_nbrs %>% 
  mutate(logit_probs = map2(training, testing, fit_logit))

brew_nbrs

```

Three questions come to mind: 

1. Which neighbor specification imputes missing values most accurately?

2. Which neighbor specification provides the most accurate set of predictied diabetes probabilities in the testing set?




```{r}

trn_missing <- diab_missing[split$in_id, ]
trn_complete <- diab_complete[split$in_id, ]

tst_missing  <- diab_missing[-split$in_id, ]
tst_complete <- diab_complete[-split$in_id, ]


impute_acc <- map_dfr(
  .x = brew_nbrs$training,
  .f = sip,
  data_missing = trn_missing,
  data_complete = trn_complete,
  .id = 'impute'
)

impute_acc %>% 
  mutate(impute = as.numeric(impute)) %>% 
  filter(variable %in% c('gender', 'frame', 'waist', 'hdl')) %>% 
  ggplot(aes(x=impute, y=score)) + 
  geom_point() + 
  geom_smooth() + 
  facet_wrap(~variable)

best_nbrs <- impute_acc %>% 
  group_by(variable) %>% 
  filter(score >= max(score))


best_trn <- map2_dfc(
  best_nbrs$impute,
  best_nbrs$variable,
  .f = ~ {
    brew_nbrs %>% 
      dplyr::filter(impute == .x) %>% 
      tidyr::unnest(cols = training) %>% 
      dplyr::select_at(tidyselect::all_of(.y))
  }
) %>% 
  bind_cols(select(trn_missing, diabetes), .)

best_tst <- map2_dfc(
  best_nbrs$impute,
  best_nbrs$variable,
  .f = ~ {
    brew_nbrs %>% 
      dplyr::filter(impute == .x) %>% 
      tidyr::unnest(cols = testing) %>% 
      dplyr::select_at(tidyselect::all_of(.y))
  }
) %>% 
  bind_cols(select(tst_missing, diabetes), .)



best_probs <- fit_logit(best_trn, best_tst)$.pred_Yes
best_score <- roc_auc_vec(tst_missing$diabetes, best_probs)


```




```{r}

results <- brew_nbrs %>% 
  transmute(
    args,
    auc = map2_dbl(
      .x = logit_probs,
      .y = testing,
      .f = ~ roc_auc_vec(estimate = .x$.pred_Yes, truth = .y$diabetes)
    )
  ) %>% 
  unnest_wider(col = args)

results

```



```{r}

ggplot(results) + 
  aes(x = k_neighbors, y = auc) + 
  geom_line(col = 'grey') + 
  geom_point(shape = 21, size = 2.4, col = 'black', fill = 'red') + 
  geom_hline(yintercept = best_score, col = 'black', linetype = 2) +
  theme_classic() +
  theme(text = element_text(size = 12)) +
  labs(y = 'Area underneath ROC curve',
    x = 'Number of neighbors used to impute missing values')

```



