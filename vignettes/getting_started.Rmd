---
title: "Getting started with `ipa`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{getting_started}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Overview

The `ipa` package helps create multiply imputed datasets for prediction models. The datasets are differentiated by specific tuning parameters that correspond to the imputation technique. Analysts can identify a single missing data strategy that works best for develop an ensembling method to combine different data strategies. 

True to its name, `ipa` has a workflow based around the process of brewing. The main steps involved are

1. `brew` create a container to hold your imputations

2. `spice` set parameters that govern the number of imputations

3. `mash` set parameters that govern imputation models

4. `ferment` impute missing values in training and (optional) testing data 

5. `bottle` output the imputed datasets as tibbles or matrices.

6. `sip` Evaluate the accuracy of imputed values (requires observed data)

# Diabetes data

These data are courtesy of Dr John Schorling, Department of Medicine, University of Virginia School of Medicine. The data originally described 1046 participants who were interviewed in a study to understand the prevalence of obesity, diabetes, and other cardiovascular risk factors in central Virginia for African Americans. A total of 403 participants were screened for diabetes, which was defined as glycosolated hemoglobin > 7.0. Among those 403 participants, 366 provided complete data for the variables in these data.

# Problem description

We'll work with the complete (`diab_complete`) and incomplete (`diab_incomplete`) versions of the `diabetes` data described above. `diab_incomplete` has about 40% missing values in each column except for diabetes status, which is complete. Missing values occur completely at random. We'll develop a prediction model to classify whether someone has prevalent type II diabetes based on clinical predictors. 

```{r, message = FALSE}

library(ipa)
library(dplyr)
library(purrr)
library(rsample)
library(parsnip)
library(rsample)
library(ggplot2)
library(yardstick)

# random seed for reproducing these results
set.seed(329)

data("diabetes", package = 'ipa')

diab_complete <- diabetes$complete
diab_missing <- diabetes$missing

```

# Problem description

A common task for prediction model development is handling missing values. `ipa` helps analysts access more options as they engage with this task. This vignette shows some of `ipa`'s features in the context of imputing missing values in the `diabetes` data. Here is a glimpse of the complete version of the data:

```{r}

glimpse(diab_complete)

```

And here is the amputed data, missing roughly 40% of the original values.

```{r}

glimpse(diab_missing)

```

We'll work with a training set and testing set in this example:

```{r}

split <- initial_split(diab_missing, prop = 1/2, strata = 'diabetes')

split

```


# Brew

`ipa_brew` objects are created using `brew_<flavor>` functions. These object are lists containing 

1. `data`: training data with missing values 
1. `pars`: tuning parameters for imputation.
1. `lims`: upper and lower bounds for `pars`
1. `wort`: a container for imputed data (initially empty)

Printing your `brew` object will show the original data with missing values, along with some additional information about the `brew`'s flavor.

```{r}

brew_nbrs <- brew_nbrs(data = training(split), outcome = diabetes)

brew_nbrs

```

Here we use `brew_nbrs` to create `brew_nbrs`. This function initiates an object that will use k-nearest neighbors to impute missing values. Other brewing functions include `brew_soft` for soft imputation and `brew_rngr` for imputation using `missRanger`.

# Spice

`spice` lets you control how many datasets are created and the primary tuning parameter for the given imputation flavor. For nearest neighbor imputation, the primary tuning parameter is the number of nearest neighbors.

The `spice` function has an argument called `with` that depends on your brew's flavor. Basically, you want to use `with = spicer_<flavor>` for a `<flavor>` brew. Following that recipe, we use `spicer_nbrs` for our `nbrs` brew.

```{r}

spcd_brew <- spice(brew_nbrs, with = spicer_nbrs(k_neighbors = c(1:50)))

# take a look at the brew's parameters (what we added using spice)
spcd_brew$pars

```

# Mash

`mash` allows you to set secondary parameters for imputing data with your brew. Additionally, `mash` will fit the models needed to impute the `brew`'s data. For a nearest neighbors brew, we can set the function that will be used to aggregate neighbors with continuous, integer, or categorical data using `masher_nbrs`. Just like `spice`, we want to use `masher_<flavor>` for a `<flavor>` brew.

```{r}

mshd_brew <- mash(spcd_brew, with = masher_nbrs(fun_aggr_ctns = median))

mshd_brew$wort

```

The `brew`'s `wort` will be filled with imputation models (i.e., the column called `fit`) after `mash`ing. For nearest neighbors imputation, the `fit` is just the training data.

```{r}

mshd_brew$wort

```

# Ferment

`ferment` imputes missing values in testing data using models fitted to training data. `ipa` tries to adhere to a few principles when it comes to imputing missing values in testing data: 

1. Outcome columns should not be used to impute missing values in training data. The outcome column will naturally be missing in the testing data, so any imputation model that depends on accessing the outcome column will be unable to impute missing values in testing data.

2. Missing data strategies should use the training data and only the training data to impute missing values in the testing data. Some imputation strategies cannot do this by design, e.g. `softImpute`, so `ipa` applies a workaround (`ferment_stk`; see below).

3. The same strategy that was used to impute training data should also be used to impute testing data.

Following these principles (and making exceptions when needed), `ipa` implements two strategies.

**Impute with fit** Models developed from the training data are applied to create imputations for missing values in the testing data. This is 
the strategy applied by nearest neighbor imputation. 

**Stack and fit:** Training and testing data are stacked. Imputation models are re-fitted to the stacked dataset. This strategy works for any imputation method, but it also requires more time to run. We use stack and fit for soft imputation of testing data automatically. However, we do not implement routines to use stack and fit for nearest neighbors. (If you really want to use stack and fit for nearest neighbors, just make two brews - one with the training data and one with the stacked data.)  

# Bottle

After imputation, your brew is ready to be bottled. The `bottle` function will attach outcomes to imputed data in the `wort` and convert that data into a `type` of your choice (options are `'tibble'` or `'matrix'`).

```{r}

imputes <- bottle(ferm_brew_fit, type = 'tibble')

imputes

```

The printed output of the `brew` has changed now that it is bottled, but the underlying `brew` object still has the same structure. `print.ipa_brew` does this because after imputation, focus normally shifts to the imputed data rather than the original data.

# Pipes and brews

The workflow functions in `ipa` are designed to be used with the `%>%` operator. 

```{r}

brew_nbrs <- training(split) %>% 
  brew_nbrs(outcome = diabetes) %>% 
  spice(k_neighbors = 1:50) %>% 
  mash(with = masher_nbrs(fun_aggr_ctns = median)) %>% 
  ferment(data_new = testing(split)) %>% 
  bottle(type = 'tibble')

```

We now have a total of 100 imputed data sets (50 training sets and 50 testing sets). If we only want to use 2 of these (1 training set and 1 testing set) A reaonable question is: Which neighbor specification imputed missing values most accurately?

# Sip

`sip` helps you check the accuracy of imputed values. Let's address an obvious limitation: imputation accuracy can only be checked when you know what the imputed value should have been. Put another way, you can only check imputation accuracy if the data you imputed weren't really missing in the first place. 

Nevertheless, you may still be able to draw generalizable insights about a problem by subsetting your incomplete data to contain only complete observations and then artificially adding missing values (as we have done with the `diabetes` data). Once you've found a strategy that seems to work best for your simulated missing data, you can apply it to the entire incomplete dataset.

Notably, `sip` needs you to tell it how to score accuracy. This instruction is passed using the arguments 

1. `fun_ctns_error` function to score continuous variables (doubles and integers)
1. `fun_bnry_error` function to score binary variables (2 categories)
1. `fun_catg_error` function to score categorical variables (>2 categories)

You can supply your own function for different variable types or rely on the default approach, which evaluates continuous variables using an R-squared statistic and binary + categorical variables using a Kappa statistic. A benefit of the default approach is that both statistics are on the same scale and have similar interpretations.  

```{r}

trn_cplt <- diab_complete[split$in_id, ]
tst_cplt <- diab_complete[-split$in_id, ]

# sip from the bottled brew.
# (two sips total to score both training and testing)
brew_nbrs <- brew_nbrs %>% 
  sip(training, data_complete = trn_cplt) %>% 
  sip(testing, data_complete = tst_cplt)


```

Save the scores as a tibble outside of the brew so that they are easier to analyze.

```{r}


impute_acc <- brew_nbrs$wort %>% 
  select(k_neighbors = impute, training_score) %>% 
  unnest(training_score)

# here are accuracy scores when using 10 neighbors.
filter(impute_acc, k_neighbors == 10)

```

Is there an ideal number of neighbors to use for the imputation of missing values in these data? A quick but limited way to investigate this is to compute an overall accuracy score for each imputed set by averaging the scores of individual variables together.

```{r}

impute_acc_ovrl <- impute_acc %>% 
  group_by(k_neighbors) %>% 
  summarize(score_overall = mean(score, na.rm = TRUE)) %>% 
  arrange(desc(score_overall))

impute_acc_ovrl

```

It doesn't look like there is a clear 'best' value of `k_neighbors` for the aggregated score, but what about a 'best' number of neighbors for individual variables? Let's visalize the imputation accuracy of four variables as a function of the number of neighbors used for imputation.

```{r}

impute_acc %>% 
  filter(variable %in% c('gender', 'frame', 'waist', 'hdl')) %>% 
  ggplot(aes(x=k_neighbors, y=score)) + 
  geom_point() + 
  geom_smooth() + 
  facet_wrap(~variable)

```


```{r, echo = FALSE}

best_frame <- impute_acc %>% 
  filter(variable == 'frame') %>% 
  arrange(desc(score)) %>% 
  slice(1) %>% 
  pull('k_neighbors')

best_gender <- impute_acc %>% 
  filter(variable == 'gender') %>% 
  arrange(desc(score)) %>% 
  slice(1) %>% 
  pull('k_neighbors')

```

It looks like the relationship between `k_neighbors` and accuracy of imputed values is variable-dependent. For example, the Kappa statistic for `frame` is maximized when we impute missing values with `r best_frame` neighbors, whereas the Kappa statistic for gender is maximized using `r best_gender`. Additionally, imputation of `waist` is fairly accurate, but imputation of `hdl` is generally inaccurate. 

# Blend

We may need to use different values of `k_neighbor` for each variable to maximize the accuracy of our imputed dataset. The easiest way to do this is with the `blend` function. There are three variants of blend:

- `blend_recipe` creates a dataframe that describes how to blend imputed data.
- `blend_training` creates a single dataframe from the training imputes.
- `blend_testing` creates a single dataframe from the testing imputes.

```{r}

recipe_nbrs <- blend_recipe(brew_nbrs)

# impute shows which value of k_neighbors maximized score for each variable
recipe_nbrs

# an imputed training set is created by plugging a recipe into blend_training.
bb_trn <- blend_training(brew_nbrs, recipe = recipe_nbrs)
# an imputed test set can be created using blend_testing
bb_tst <- blend_testing(brew_nbrs, recipe = recipe_nbrs)

# note: bb stands for best brew

```

# Modeling

Another interesting question is whether any of the 50 pairs of imputed datasets will provide a better set of data for developing and evaluating prediction models. Let's fit a `parsnip` logistic regression model to the imputed training sets and then compute predictions on the corresponding imputed testing sets.

```{r}

# save the imputed data to an object with a better name
imputes <- brew_nbrs$wort

# a function to fit logit model, and predict on new data
fit_logit <- function(df_train, df_test){
  logistic_reg() %>% 
    set_engine('glm') %>% 
    fit(diabetes ~ ., data = df_train) %>% 
    predict(new_data = df_test, type = 'prob')
}

imputes <- imputes %>% 
  mutate(logit_probs = map2(training, testing, fit_logit),
    truth = map(testing, 'diabetes')) %>% 
  select(impute, truth, logit_probs)

imputes

```

Let's go through these predictions.


```{r}

bb_probs <- fit_logit(bb_trn, bb_tst)$.pred_Yes

# compute the AUC for these predictions
bb_auc <- roc_auc_vec(bb_tst$diabetes, bb_probs)

```

Now we can look at the area underneath the ROC curve for each of the 50 neighbor values we used to impute our data as well as the 'best brew' we created with `sip` and `blend`.

```{r}

results <- imputes %>% 
  transmute(
    impute,
    auc = map2_dbl(
      .x = logit_probs,
      .y = truth,
      .f = ~ roc_auc_vec(estimate = .x$.pred_Yes, truth = .y)
    )
  ) 

results

```



```{r}

ggplot(results) + 
  aes(x = impute, y = auc) + 
  geom_line(col = 'grey') + 
  geom_point(shape = 21, size = 2.4, col = 'black', fill = 'red') + 
  geom_hline(yintercept = bb_auc, col = 'black', linetype = 2) +
  theme_classic() +
  theme(text = element_text(size = 12)) +
  labs(y = 'Area underneath ROC curve',
    x = 'Number of neighbors used to impute missing values')

```



