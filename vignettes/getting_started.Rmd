---
title: "Getting started with `ipa`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{getting_started}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The `ipa` package helps create collections of imputed datasets using a specific method. The datasets are differentiated by specific tuning parameters that correspond to the method. For example, the number of neighbors used to impute missing values is a specific parameter for nearest neighbor imputation. True to its name, `ipa` is based around the process of brewing a great beer. The main steps involved are

1. `brew` Create a container to hold your imputations

2. `spice` (optional) set parameters that govern the number of imputations for the given brew

3. `mash` fit models that will provide imputations

4. `ferment` impute missing values in training and (optionally) testing data 

5. `bottle` output the imputed datasets in a tibble.

## Simulated Example

```{r}

library(ipa)
library(magrittr)
library(recipes)
library(dplyr)
library(purrr)
library(ggplot2)

# random seed for reproducing these results
set.seed(329)

# simulation parameters (these are explained below)
ncov = 50
nint = 0
error_sd = 1/5
tst_miss = 0.0
trn_miss = 0.50
nobs = 5000
split_prop = 1/2
rho = 1/2
nimpute = 10

```

As this is a simulation example, we'll need to set a few parameters, describe the context we are simulating data for, and then generate our data. Suppose that

- $Y = X\beta + \epsilon$, where $Y$ is a continuous outcome, $X$ is a matrix with `ncov` = `r ncov` covariates, $\beta$ is a vector containing coefficients that correspond to each column in $X$, and $\epsilon$ is a random error term with standard deviation = `r error_sd`. `nint` = `r nint` is the number of interactions between the columns in $X$ that impact the value of $Y$.

- Columns in $X$ follow an autoregressive correlation structure characterized by a correlation constant (`rho` = `r rho`) such that corr$(x_i, x_j)$ =  `rho`$^{\left|i-j\right|}$, where $i$ and $j$ indicate column indices in $X$.

- We sample a proportion of these data (`split_prop` = `r split_prop`) from a population of size `nobs` = `r format(nobs, big.mark=',')`. 

- A proportion (`trn_miss` = `r trn_miss`) of the values in our sample are missing at random (i.e., the missing status of a given variable is related to other variables we measured).

- We want to identify a strategy to handle these missing values that will optimize the accuracy of a gradient boosted decision tree ensemble, which will be constructed using `xgboost`. 

- We will test the accuracy of each `xgboost` model by using it to predict $Y \mid X$ among the full population (minus the observations in our training data).

In the following code, I set the parameters described above and use `gen_simdata` to create a set of 50,000 observations (with 2,500 in our training sample)

```{r}

sim <- gen_simdata(
  problem_type = 'regression', # continuous outcome.
  error_sd = error_sd,         # standard deviation of error.
  ncov = ncov,                 # number of predictors.
  nint = nint,                 # number of interactions.
  rho = rho,                   # autoregression constant.
  nobs = nobs,                 # total No. of observations.
  split_prop = split_prop,     # proportion used to train model.
  miss_pattern = 'mar',        # data are missing at random.
  trn_miss_prop = trn_miss,    # proportion of missing training data.
  tst_miss_prop = tst_miss     # proportion of missing training data.
)

# save the vector of beta coefficients
betas <- sim[1]
data <- sim[2:3]

print(data)

```

## Standard approach

Imputation to the mean is a common strategy used to impute missing values for predictive models. We will use the performance of a linear model fitted to mean imputed data as a reference point for this example. 

```{r}

reci <- recipe(response ~ ., data = data$training) %>% 
  step_meanimpute(all_numeric()) %>% 
  prep()

# Model, predictions, and mean squared error
# using mean imputation (the reference approach)

ref_mdl <- lm(response ~ ., data = juice(reci))
ref_prd <- predict(ref_mdl, newdata = bake(reci, data$testing))
ref_mse <- mean((ref_prd - data$testing$response)^2)

```


## `softImpute`

soft imputation is a method based on singular value decomposition of a matrix. We will use the `softImpute` algorithm to brew a set of 20 imputed datasets with varying degrees of regularization. 

### Brew

When you start a brew, an object is initiated with an empty list containing meta data about your missing data strategy. 

```{r}

brew_sft <- data$training %>% 
  brew(outcome = response, flavor = 'softImpute')

brew_sft

```


### Spice (if you want to)

If you are going to make all these imputed datasets, you might as well make them how you like them. Spicing your brew gives you more control over how many datasets are created and the values of parameters that will be used to generate them.

Of course, different brews take different spices, and it is a little overwhelming trying to remember which spice goes where. For this reason, `ipa` includes helper functions (see `spicer_soft`, `spicer_nbrs`, and `spicer_rngr`) to help add the correct arguments into the `spice` function via the `with` input. 

```{r}

brew_sft <- brew_sft %>% 
  spice(with = spicer_soft(n_impute = 24, step_size = 2))

# Note that I chose step_size so that n_impute * step_size < max_rank
# (spice would throw an error at me if I messed that up)

brew_sft$pars

```


### Mash

Mashing the brew corresponds to the initiation of imputation for `ipa_brew` objects. Inputs of the `mash` function include parameters that directly correspond to imputation models. Again, different models take different parameters, so we rely on the `masher_soft` function to help us set parameters that matter for the `softImpute` algorithm.

Notably, the `softImpute` algorithm's convergence is influenced by parameters set in the `mash` function, and it is often helpful to see model fitting output printed to the screen to diagnose convergence problems. To see that output, we use the `set_verbose` function and make our brew a little noisier.

```{r}

brew_sft <- brew_sft %>% 
  verbose_on(level = 1) %>% 
  mash(with = masher_soft(scale_lambda = 0.90))

```

The `ipa_brew` object's `wort` will be filled with imputation models (i.e., the column called `fit`) after the mash is complete!

```{r}

brew_sft$wort

```

### Ferment

Missing values can occur in the training data but also the testing data. There are two few principles that `ipa` follows regarding missing values in testing data. 

1. Outcome columns should not be used to impute missing values. *Why?* First, the outcome column will naturally be missing in the testing data, so any imputation model that depends on accessing the outcome column will be unable to impute missing values in testing data. Second, foregoing the use of any outcome column in imputation models allows analysts to impute data and then apply cross-validation to tune downstream predictive models rather than re-imputing the data in each step of cross-validation (the second option requires far more computation time).   

2. A missing data strategy should not fit imputation models solely to the testing data. *Why?* Sometimes testing data are a single observation - it's hard to fit a model to that. 

There are at least two ways to handle missing data according to these principles. 

**Stack and fit:** Training and testing data are stacked. Imputation models are re-fitted to the stacked dataset.

**Training neighbors** A set of nearest neighbors in the training data is identified for each observation in the testing data. Observed values for these training observations are used to impute missing values in the testing data.

In an ideal world, imputation models are fitted to the training data and then applied to the testing data without needing to be re-fit. Unfortunately, some imputation strategies are not designed to work this way! For example, `softImpute` imputes missing values based on the index of the missing value in the training data, and this doesn't generalize to testing data because testing data (by definition) are not in the training data. However, the stack and fit strategy works quite well for `softImpute`.

The `...` argument in `ferment` lets you create a new column in the `wort` of the brew using the test impute strategy of your choice. For example, we'll use `test_stkr` below to apply the stack and fit strategy:

```{r}

brew_sft <- ferment(brew_sft, testing = test_stkr(data$testing))

brew_sft$wort

```

### Bottle

Once everything is imputed, your brew is ready to be bottled. You can choose whether you'd prefer to get the data back in the form of a `tibble` or `matrix`.

```{r}

brew_sft <- bottle(brew_sft, type = 'tibble')

brew_sft

```

### A pipe with your brew

The main functions in `ipa_brew` are designed to fit neatly with the `%>%` operator. We know you like to pipe while you brew.

```{r, eval = FALSE}

brew_sft <- data$training %>% 
  brew(outcome = response, flavor = 'softImpute') %>% 
  verbose_on(level = 1) %>% 
  spice(with = spicer_soft(n_impute = 24, step_size = 2)) %>% 
  mash(with = masher_soft(scale_lambda = 0.90)) %>% 
  ferment(testing = test_nbrs(data$testing)) %>% 
  bottle(type = 'tibble')

```

Now that we have our data, we can fit a linear model to each training set, and evaluate that model's predictions using the testing data. Our main questions are 

(1) can softImpute produce a model that is more accurate than mean imputation? 

(2) What are the ideal parameters for softImpute in this case?

```{r}

trn_yhat <- mean(data$training$response)
trn_mse <- mean((trn_yhat - data$testing$response)^2)

ggdat <- brew_sft %>% 
  mutate(
    sft_mse = map2_dbl(training, testing,
      .f = ~ lm(response ~ ., data = .x) %>% 
        predict(newdata = .y) %>% 
        subtract(data$testing$response) %>% 
        raise_to_power(2) %>% 
        mean()
    )
  ) %>% 
  select(impute, sft_mse) %>% 
  mutate(r2_sft = 1 - sft_mse / trn_mse)

```

Results show that all instances of softImpute have an advantage over mean imputation and that imputes 16-21 seem to provide the most accurate linear models. Using these imputed datasets rather than the reference approach (imputation to the mean) improves the testing r-squared statistic by approximately `r round(max(ggdat$r2_sft) - (1 - ref_mse / trn_mse), 3)` 

```{r, fig.width=7, fig.height=5}

ggplot(ggdat, aes(x = impute, y = r2_sft)) + 
  geom_line(col = 'grey') +
  geom_point(size = 2, shape = 21, fill = 'red', col = 'grey') + 
  geom_hline(yintercept = 1 - ref_mse / trn_mse, linetype = 2) +
  labs(x = 'Imputation', y = 'Testing R-squared statistic') + 
  theme_bw() + 
  theme(panel.grid = element_blank())

```


