---
title: "Getting started with `ipa`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{getting_started}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Overview

The `ipa` package helps create collections of imputed datasets using a specific method. The datasets are differentiated by specific tuning parameters that correspond to the method. For example, the number of neighbors used to impute missing values is a specific parameter for nearest neighbor imputation. True to its name, `ipa` is based around the process of brewing a great beer. The main steps involved are

1. `brew` Create a container to hold your imputations

2. `spice` (optional) set parameters that govern the number of imputations for the given brew

3. `mash` fit models that will provide imputations

4. `ferment` impute missing values in training and (optionally) testing data 

5. `bottle` output the imputed datasets in a tibble.

# Diabetes data

These data are courtesy of Dr John Schorling, Department of Medicine, University of Virginia School of Medicine. The data originally described 1046 participants who were interviewed in a study to understand the prevalence of obesity, diabetes, and other cardiovascular risk factors in central Virginia for African Americans. A total of 403 participants were screened for diabetes, which was defined as glycosolated hemoglobin > 7.0. Among those 403 participants, 366 provided complete data for the variables in these data.

We'll work with the complete (`diab_complete`) and incomplete (`diab_incomplete`; , missingness added artificially and completely at random) versions of the `diabetes` data here, and we'll focus on developing a prediction model to classify whether someone has prevalent type II diabetes based on clinical risk factors. 

```{r}

library(ipa)
library(dplyr)
library(purrr)
library(ggplot2)

# random seed for reproducing these results
set.seed(329)

data("diabetes", package = 'ipa')

diab_complete <- diabetes$complete
diab_missing <- diabetes$missing

```

## Problem description

A common task for prediction model development is handling missing values. `ipa` helps analysts access more options as they engage with this task. This vignette shows some of `ipa`'s features in the context of imputing missing values in the `diabetes` data. Here is a glimpse of the complete version of the data:

```{r}

glimpse(diab_complete)

```

And here is the amputed data, missing roughly 40% of the original values.

```{r}

glimpse(diab_missing)

```

# Brew

To start your workflow with `ipa`, initiate a brew using `brew_<flavor>`. The object created is essentially a list containing meta data about your missing data strategy. However, printing your `brew` object will always show the original data, because that's usually more helpful than overwhelming the console with all of the list elements. 

```{r}

diab_brew <- brew_nbrs(data = diab_missing, outcome = diabetes)

diab_brew

```

Here we use `brew_nbrs` to create `diab_brew`. This function initiates an object that will use k-nearest neighbors to impute missing values. Other brewing functions include `brew_soft` for soft imputation and `brew_rngr` for imputation using `missRanger`.

# Spice

`spice` lets you control how many datasets are created and the primary tuning parameter for the given imputation flavor. For nearest neighbor imputation, the primary tuning parameter is the number of nearest neighbors.

The `spice` function has an argument called `with` that depends on your brew's flavor. Basically, you want to use `with = spicer_<flavor>` for a `<flavor>` brew. Following that recipe, we use `spicer_nbrs` for our `nbrs` brew.

```{r}

spcd_brew <- spice(diab_brew, with = spicer_nbrs(neighbors = c(1:50)))

# take a look at the brew's parameters (what we added using spice)
spcd_brew$pars

```

# Mash

`mash` allows you to set secondary parameters for imputing data with your brew. Additionally, `mash` will fit the models needed to impute the `brew`'s data. For a nearest neighbors brew, we can set the function that will be used to aggregate neighbors with continuous, integer, or categorical data using `masher_nbrs`. Just like `spice`, we want to use `masher_<flavor>` for a `<flavor>` brew.

```{r}

mshd_brew <- mash(spcd_brew, with = masher_nbrs(fun_aggr_ctns = median))

mshd_brew$wort

```

The `brew`'s `wort` will be filled with imputation models (i.e., the column called `fit`) after `mash`ing:

```{r}

mshd_brew$wort

```

# Ferment

Missing values can occur in the training data but also in testing data. `ipa` tries to adhere to a few principles when it comes to imputing missing values in testing data: 

1. Outcome columns should not be used to impute missing values in training data. The outcome column will naturally be missing in the testing data, so any imputation model that depends on accessing the outcome column will be unable to impute missing values in testing data. This is why `outcome` is a required input for `brew`

2. Missing data strategies should use the training data and only the training data to impute missing values in the testing data. Some imputation strategies cannot do this by design, e.g. `softImpute`, so `ipa` applies a workaround (see below).  

3. The same strategy that was used to impute training data should also be used to impute testing data.

Following these principles (and making exceptions when needed), `ipa` has two broad strategies to implement.

**Impute with fit** Models developed from the training data are applied to create imputations for missing values in the testing data.

**Stack and fit:** Training and testing data are stacked. Imputation models are re-fitted to the stacked dataset. This strategy works for any imputation method, but may give overly optimistic results if the testing set is much larger than the training set. 

For nearest neighbors,  A set of nearest neighbors in the training data is identified for each observation in the testing data. Observed values for these training observations are used to impute missing values in the testing data.

In an ideal world, imputation models are fitted to the training data and then applied to the testing data without needing to be re-fit. Unfortunately, some imputation strategies are not designed to work this way! For example, `softImpute` imputes missing values based on the index of the missing value in the training data, and this doesn't generalize to testing data because testing data (by definition) are not in the training data. However, the stack and fit strategy works quite well for `softImpute`.

The `...` argument in `ferment` lets you create a new column in the `wort` of the brew using the test impute strategy of your choice. For example, we'll use `test_stkr` below to apply the stack and fit strategy:

```{r}

brew_sft <- ferment(brew_sft, testing = test_stkr(data$testing))

brew_sft$wort

```

### Bottle

Once everything is imputed, your brew is ready to be bottled. You can choose whether you'd prefer to get the data back in the form of a `tibble` or `matrix`.

```{r}

brew_sft <- bottle(brew_sft, type = 'tibble')

brew_sft

```

### A pipe with your brew

The main functions in `ipa_brew` are designed to fit neatly with the `%>%` operator. We know you like to pipe while you brew.

```{r, eval = FALSE}

brew_sft <- data$training %>% 
  brew(outcome = response, flavor = 'softImpute') %>% 
  verbose_on(level = 1) %>% 
  spice(with = spicer_soft(n_impute = 24, step_size = 2)) %>% 
  mash(with = masher_soft(scale_lambda = 0.90)) %>% 
  ferment(testing = test_nbrs(data$testing)) %>% 
  bottle(type = 'tibble')

```

We will fit a `parsnip` logistic regression model to these data after imputing missing values. Here is the function we'll apply to do that:

```{r}

fit_logit <- function(df_train, df_test){
  logistic_reg() %>% 
    set_engine('glm') %>% 
    fit(diabetes ~ ., data = df_train) %>% 
    predict(new_data = df_test, type = 'prob')
}

```

Two questions stand out: 

1. Which imputation technique most accurately imputes missing values?

2. Which imputation technique provides the most accurate logistic regression model?

# 1. Imputation accuracy

# 2. Model accuracy

```{r}

n_imp <- 10
ls <- seq(25, 1, length.out = n_imp)

resamples <- vfold_cv(diab_missing, strata = 'diabetes')

resamples <- resamples %>% 
  mutate(
    soft_imp = map(
      .x = splits, 
      .f =  ~ {
        brew_soft(data = training(.x), outcome = diabetes) %>% 
          spice(with = spicer_soft(n_impute = n_imp, step_size = 1)) %>% 
          mash(with = masher_soft(scale_iter = 50, lambda_sequence = ls)) %>% 
          ferment(testing = test_stkr(testing(.x))) %>% 
          bottle(type = 'tibble')
      }
    ),
    nbrs_imp = map(
      .x = splits, 
      .f = ~ {
        brew_nbrs(data = training(.x), outcome = diabetes) %>% 
          spice(with = spicer_nbrs(neighbors = seq(6, 20))) %>% 
          mash() %>% 
          ferment(testing = test_nbrs(testing(.x))) %>% 
          bottle(type = 'tibble')
      }
    )
  )


resamples

```


```{r}

model_data <- resamples %>% 
  select(id, ends_with('imp')) %>% 
  pivot_longer(cols = ends_with('imp')) %>% 
  unnest(cols = value)

model_data

```



```{r}

model_data <- model_data %>% 
  mutate(
    training = map(training, mutate, diabetes = factor(diabetes)),
    testing = map(testing, mutate, diabetes = factor(diabetes)),
    logit_predprob = map2(
      .x = training, 
      .y = testing, 
      .f = fit_logit
    )
  )

```


```{r}

results_data <- model_data %>%
  mutate(
    probs = map2(
      .x = testing, 
      .y = logit_predprob,
      .f = ~ bind_cols(truth = .x$diabetes, .y)
    )
  ) %>% 
  group_by(name, impute) %>% 
  summarize(
    probs = list(bind_rows(probs))
  )

```

```{r}

results <- results_data %>% 
  mutate(
    auc = map(
      .x = probs, 
      .f = yardstick::roc_auc, .pred_Yes, truth = truth
    )
  ) %>% 
  unnest_wider(col = auc)

```

```{r}

ggplot(results) + 
  aes(x = impute, y = .estimate) + 
  facet_wrap(~name) + 
  geom_line(col = 'grey') + 
  geom_point(shape = 21, col = 'black', fill = 'red')

```


## Standard approach

Imputation to the mean is a common strategy used to impute missing values for prediction models. We will use the performance of a linear model fitted to mean imputed data as a reference point for this example. 

```{r}


tmp = resamples %>% 
  mutate(
    ref = map_dbl(
      .x = splits, 
      .f = ~ {

        .reci <- prep(reci, training = training(.x))
        
        trn <- juice(.reci)
        tst <- bake(.reci, new_data = testing(.x))
        
        trn %<>% mutate(diabetes = factor(diabetes))
        tst %<>% mutate(diabetes = factor(diabetes))
        
        logistic_reg() %>% 
          set_engine('glm') %>% 
          fit(diabetes ~ ., data = trn) %>% 
          predict(new_data = tst, type = 'prob') %>% 
          yardstick::roc_auc(.pred_1, truth = tst$diabetes) %>% 
          pull(.estimate)
          
      }
    )
  )

# Model, predictions, and mean squared error
# using mean imputation (the reference approach)

ref_mdl <- lm(response ~ ., data = juice(reci))
ref_prd <- predict(ref_mdl, newdata = bake(reci, data$testing))
ref_mse <- mean((ref_prd - data$testing$response)^2)

```




Now that we have our data, we can fit a linear model to each training set, and evaluate that model's predictions using the testing data. Our main questions are 

(1) can softImpute produce a model that is more accurate than mean imputation? 

(2) What are the ideal parameters for softImpute in this case?

```{r}

trn_yhat <- mean(data$training$response)
trn_mse <- mean((trn_yhat - data$testing$response)^2)

ggdat <- brew_sft %>% 
  mutate(
    sft_mse = map2_dbl(training, testing,
      .f = ~ lm(response ~ ., data = .x) %>% 
        predict(newdata = .y) %>% 
        subtract(data$testing$response) %>% 
        raise_to_power(2) %>% 
        mean()
    )
  ) %>% 
  select(impute, sft_mse) %>% 
  mutate(r2_sft = 1 - sft_mse / trn_mse)

```

Results show that all instances of softImpute have an advantage over mean imputation and that imputes 16-21 seem to provide the most accurate linear models. Using these imputed datasets rather than the reference approach (imputation to the mean) improves the testing r-squared statistic by approximately `r round(max(ggdat$r2_sft) - (1 - ref_mse / trn_mse), 3)` 

```{r, fig.width=7, fig.height=5}

ggplot(ggdat, aes(x = impute, y = r2_sft)) + 
  geom_line(col = 'grey') +
  geom_point(size = 2, shape = 21, fill = 'red', col = 'grey') + 
  geom_hline(yintercept = 1 - ref_mse / trn_mse, linetype = 2) +
  labs(x = 'Imputation', y = 'Testing R-squared statistic') + 
  theme_bw() + 
  theme(panel.grid = element_blank())

```


soft imputation is a method based on singular value decomposition of a matrix. We will use the `softImpute` algorithm to brew a set of 20 imputed datasets with varying degrees of regularization. 

