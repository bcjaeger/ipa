---
title: "Getting started with `ipa`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{getting_started}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Overview

This vignette shows you how to create a `brew` object, a container for multiple imputed datasets and the primary focus of the `ipa` package. 

# Why use `ipa`?

The `ipa` package helps create multiple imputed datasets for prediction models. The datasets are differentiated by specific tuning parameters that correspond to the imputation technique. After creating these imputed sets, 

- you can identify one single missing data strategy that most accurately imputes missing data or one strategy that results in the most accurate predition model.

- You can blend imputation strategies to eek out the best possible set of imputed data for your analysis needs. 

- You can combine imputation strategies to create a framework for multiple imputation, which may provide even more accurate prediction models than a well-chosen single imputation strategy.

True to its name, `ipa` has a workflow based around the process of brewing. The main steps involved are

1. `brew`: create a container to hold your imputations

1. `spice`: set primary imputation parameters

1. `mash`: set secondary imputation parameters 

1. `stir`: fit imputation model(s) to training data

1. `ferment`: impute missing values in training and (optional) testing data 

1. `bottle`: convert imputed values to imputed sets (`tibble` or `matrix`)

1. `sip`: evaluate accuracy of imputed values for each variable, separately. (requires observed data).

1. `chug`: evaluate accuracy of downstream prediction models.

1. `distill`: reduce multiple imputed sets into one (based on accuracy from `sip` or `chug`)

1. `flight`: subset the multiple imputed sets into a smaller set optimized for prediction accuracy (in development).

# Diabetes data

These data are courtesy of Dr John Schorling, Department of Medicine, University of Virginia School of Medicine. The data originally described 1046 participants who were interviewed in a study to understand the prevalence of obesity, diabetes, and other cardiovascular risk factors in central Virginia for African Americans. A total of 403 participants were screened for diabetes, which was defined as glycosolated hemoglobin > 7.0. Among those 403 participants, 366 provided complete data for the variables in these data.

# Problem description

We'll work with the complete (`diab_complete`) and incomplete (`diab_incomplete`) versions of the `diabetes` data described above. `diab_incomplete` has about 40% missing values in each column except for diabetes status, which is complete. Missing values occur completely at random. We'll develop a prediction model to classify whether someone has prevalent type II diabetes based on clinical predictors. 

```{r, message = FALSE}

library(ipa)
library(dplyr)
library(purrr)
library(yardstick)

data("diabetes", package = 'ipa')

diab_complete <- diabetes$complete
diab_missing <- diabetes$missing

```

# Problem description

A common task for prediction model development is handling missing values. `ipa` helps analysts access more options as they engage with this task. This vignette shows some of `ipa`'s features in the context of imputing missing values in the `diabetes` data. Here is a glimpse of the complete version of the data:

```{r}

glimpse(diab_complete)

```

And here is the amputed data, missing roughly 40% of the original values.

```{r}

glimpse(diab_missing)

```

We'll work with a training set and testing set in this example:

```{r}

# random seed for reproducing these results
set.seed(730)
train_index <- sample(nrow(diab_missing), 200)

training <- diab_missing[train_index, ]
testing  <- diab_missing[-train_index, ]

```


# Brew

`ipa_brew` objects are created using `brew_<flavor>` functions. These object are lists containing 

1. `data`: training data with missing values 
1. `pars`: tuning parameters for imputation.
1. `lims`: upper and lower bounds for `pars`
1. `wort`: a container for imputed data (initially empty)

Printing your `brew` object will show the original data with missing values, along with some additional information about the `brew`'s flavor.

```{r}

brew_init <- brew_nbrs(data = training, outcome = diabetes)

brew_init

```

Here we use `brew_nbrs` to create `brew_init`. This function initiates an object that will use k-nearest neighbors to impute missing values. Other brewing functions include `brew_soft` for soft imputation and `brew_rngr` for imputation using `missRanger`.

# Spice

`spice` lets you control the primary parameters for imputing data with your brew. By primary parameters, we mean the parameters that govern how many imputed datasets are created. For nearest neighbor imputation, the primary tuning parameter is the number of nearest neighbors.

The `spice` function has an argument called `with` that depends on your brew's flavor. The general idea is to use `with = spicer_<flavor>` for a `<flavor>` brew, so we use `spicer_nbrs` for our `nbrs` brew. The advantage of using `spicer` functions is that, if you use Rstudio, you can hit the tab key and automatically see a list of the relevant parameters for your brew.  

For brewing veterans, `spice` also contains a `...` input that lets you specific parameters directly instead if using `with = spicer_nbrs()`. This lets you write more concise code if you already know what parameters go with your brew's flavor and you don't need any help remembering them. To clarify this, we use the `spice` function both ways below. 

```{r}

# how to spice if you want help remembering what parameters to use
spcd_brew <- spice(brew_init, with = spicer_nbrs(k_neighbors = c(1:50)))

# a more concise spice for those who don't need auto-completion
spcd_brew <- spice(brew_init, k_neighbors = 1:50)

# take a look at the brew's parameters (what we added using spice)
spcd_brew$pars

```

# Mash

`mash` allows you to set secondary parameters for imputing data with your brew. Additionally, `mash` fits the models and adds their imputed values to the `wort` component of the `brew`. By secondary, we mean parameters that do not impact the number of imputed datasets created. 

For a nearest neighbors brew, `mash` allows us to set functions that will be used to aggregate continuous, integer, or categorical values from $k$ nearest neighbors. Just like `spice`, we can use `masher_<flavor>` for a `<flavor>` brew, or we can write input arguments directly using `...`.

```{r}

# how to mash if you want help remembering what parameters to use
mshd_brew <- mash(spcd_brew, with = masher_nbrs(fun_aggr_ctns = median))

# a more concise mashing call if you don't need help remembering parameters.
mshd_brew <- mash(spcd_brew, fun_aggr_ctns = median)

mshd_brew

```

# Stir

Once the parameters are set, it's time to fit imputation models with `stir`. This step will probably require more computation time compared to other steps in the `brew` workflow. the `timer` argument can be set to `TRUE` for this function in case you'd like to monitor time taken to fit the models used for imputation.

```{r}

stirred_brew <- stir(mshd_brew, timer = TRUE)

stirred_brew

```


The printed output of the `brew` has changed. Originally, printing our `brew` showed the data we wanted to impute, but now printing the brew object shows a `tibble` comprising our imputed data. Notably, the `brew` object's structure has not changed (it is still a list with four things), but `print.ipa_brew()` now shows `brew$wort` instad of `brew$data`. We've organized `print` to work like this because focus normally shifts to the imputed data after running imputation.


# Ferment

`ferment` imputes missing values in testing data and only has two arguments (a `brew` and a data frame with missing values), making it very simple to use: 

```{r}

ferm_brew <- ferment(stirred_brew, data_new = testing)

ferm_brew

```

Under the hood, `ferment` does a lot of opinionated work based on the following general principles: 

1. _Missing data strategies should use the training data to impute missing values in the testing data and should never use the testing data to impute the training data._ `ipa` applies two methods to follow this principle:

- _Impute with fit:_ Models developed from the training data are applied to create imputations for missing values in the testing data. This is 
the strategy applied by nearest neighbor imputation. Some imputation strategies cannot do this by design (e.g. `softImpute`), and `ipa` applies the 'stack and fit' method for these.

- _Stack and fit:_ Training data are imputed using only the training data. To impute testing data, the unimputed training and testing data are stacked. The same imputation models that were used to impute training data are re-fitted to the stacked dataset, and testing data are imputed based on these models. This strategy may require a lot more computing time than 'impute with fit', and is therefore only applied when needed.

2. _Outcome columns should not be used to impute missing values in training data._ Outcome columns will naturally be missing in the testing data, so any missing data strategy that depends on accessing an outcome column will struggle to impute missing values in testing data.  

3. The same modeling strategy that was used to impute training data should also be used to impute testing data. In other words, we don't recommend imputing the training data with `softImpute` and then imputing the testing data using nearest neighbors. 

_What if you don't agree with these principles?_ It is very hard to break from these principles using functions in the `brew` family (`brew`, `spice`, `mash`, `ferment`, `bottle`, `sip`, `chug`, and `distill`.). However, the more flexible `ipa` functions (e.g., `impute_nbrs`, `impute_soft`) can break these principles if you ask them to.

# Bottle

After imputation, your brew is ready to be bottled. The `bottle` function will

- replace the lists of imputed values with lists of imputed data sets in the `wort` of the brew. 

- bind the outcome column(s) with these imputed datasets.

- format the imputed datasets into `tibble` or `matrix` composition.

```{r}

imputes <- bottle(ferm_brew, type = 'tibble')

imputes

```

# Pipes and brews

The `brew` functions in `ipa` are designed to be used with the `%>%` operator and the `training()` / `testing()` functions in the `rsample` package. A typical workflow for creating a brew looks like the code below:

```{r}

imputes <- training %>%           # start with training data
  brew_nbrs(outcome = diabetes) %>%      # initialize brew; separate outcomes
  spice(k_neighbors = 1:50) %>%          # set primary tuning parameters
  mash(fun_aggr_ctns = median) %>%       # set secondary tuning parameters
  stir(timer = TRUE) %>%                 # impute training data
  ferment(data_new = testing) %>% # impute testing data
  bottle(type = 'tibble')                # turn imputed values into datasets

```

We now have a total of 100 imputed data sets (50 training sets and 50 testing sets). In the next vignette, we will talk about `sip`, `chug`, and `distill`, which will help you assess your imputation strategies, identify an optimal strategy for your primary analysis, and create one (or multiple) imputed dataset(s) to do that analysis.

